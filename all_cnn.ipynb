{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveRelu(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdaptiveRelu, self).__init__()\n",
    "        self.thr = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.LeakyReLU()(x - self.thr) + self.thr\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Sequential\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement a simple custom module that reshapes (n, m, 1, 1) tensors to (n, m).\n",
    "    \"\"\"\n",
    "    def forward(self,x):\n",
    "        n,m,i,j = x.size()\n",
    "        return x.view(n,m)\n",
    "        \n",
    "\n",
    "def all_cnn_module():\n",
    "    \"\"\"\n",
    "    Create a nn.Sequential model containing all of the layers of the All-CNN-C as specified in the paper.\n",
    "    https://arxiv.org/pdf/1412.6806.pdf\n",
    "    Use a AvgPool2d to pool and then your Flatten layer as your final layers.\n",
    "    You should have a total of exactly 23 layers of types:\n",
    "    - nn.Dropout\n",
    "    - nn.Conv2d\n",
    "    - nn.ReLU\n",
    "    - nn.AvgPool2d\n",
    "    - Flatten\n",
    "    :return: a nn.Sequential model\n",
    "    \"\"\"\n",
    "    return Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv2d(3,96,(3,3),stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(96,96,(3,3),stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(96,96,(3,3),stride=(2, 2), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(96,192,(3,3),stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(192,192,(3,3),stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(192,192,(3,3),stride=(2, 2), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(192,192,(3,3),stride=(1,1), padding=(0, 0)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(192,192,(1,1),stride=(1, 1), padding=(0, 0)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(192,10,(1,1),stride=(1, 1), padding=(0, 0)),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(6,6),\n",
    "            Flatten()\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample_zero_mean(x):\n",
    "    \"\"\"\n",
    "    Make each sample have a mean of zero by subtracting mean along the feature axis.\n",
    "    :param x: float32(shape=(samples, features))\n",
    "    :return: array same shape as x\n",
    "    \"\"\"\n",
    "    x = x.T - np.mean(x, axis=1)\n",
    "    return x.T\n",
    "\n",
    "def gcn(x, scale=55., bias=0.01):\n",
    "    \"\"\"\n",
    "    GCN each sample (assume sample mean=0)\n",
    "    :param x: float32(shape=(samples, features))\n",
    "    :param scale: factor to scale output\n",
    "    :param bias: bias for sqrt\n",
    "    :return: scale * x / sqrt(bias + sample variance)\n",
    "    \"\"\"\n",
    "    var = np.var(x, axis=1)\n",
    "    GCN = scale * x.T / np.sqrt(bias + var)\n",
    "    return GCN.T\n",
    "\n",
    "\n",
    "def feature_zero_mean(x, xtest):\n",
    "    \"\"\"\n",
    "    Make each feature have a mean of zero by subtracting mean along sample axis.\n",
    "    Use train statistics to normalize test data.\n",
    "    :param x: float32(shape=(samples, features))\n",
    "    :param xtest: float32(shape=(samples, features))\n",
    "    :return: tuple (x, xtest)\n",
    "    \"\"\"\n",
    "    mean = np.mean(x, axis = 0)\n",
    "    x = x - mean\n",
    "    xtest = xtest - mean\n",
    "    return x, xtest\n",
    "\n",
    "\n",
    "def zca(x, xtest, bias=0.1):\n",
    "    \"\"\"\n",
    "    ZCA training data. Use train statistics to normalize test data.\n",
    "    :param x: float32(shape=(samples, features)) (assume mean=0)\n",
    "    :param xtest: float32(shape=(samples, features))\n",
    "    :param bias: bias to add to covariance matrix\n",
    "    :return: tuple (x, xtest)\n",
    "    \"\"\"\n",
    "    d,n = x.shape\n",
    "    cov = np.dot(x.T,x)/d + np.eye(n)*bias\n",
    "    print(cov.shape)\n",
    "    U,S,V = np.linalg.svd(cov)\n",
    "    S_1 = 1./S * np.eye(n)\n",
    "    W = np.dot( np.dot(U, np.sqrt(S_1)) , V)\n",
    "    return np.dot(x,W),np.dot(xtest,W)\n",
    "\n",
    "\n",
    "def cifar_10_preprocess(x, xtest, image_size=32):\n",
    "    \"\"\"\n",
    "    1) sample_zero_mean and gcn xtrain and xtest.\n",
    "    2) feature_zero_mean xtrain and xtest.\n",
    "    3) zca xtrain and xtest.\n",
    "    4) reshape xtrain and xtest into NCHW\n",
    "    :param x: float32 flat images (n, 3*image_size^2)\n",
    "    :param xtest float32 flat images (n, 3*image_size^2)\n",
    "    :param image_size: height and width of image\n",
    "    :return: tuple (new x, new xtest), each shaped (n, 3, image_size, image_size)\n",
    "    \"\"\"\n",
    "    x = sample_zero_mean(x)\n",
    "    xtest = sample_zero_mean(xtest)\n",
    "    x = gcn(x)\n",
    "    xtest = gcn(xtest)\n",
    "    x, xtest = feature_zero_mean(x, xtest)\n",
    "    x, xtest = zca(x, xtest)\n",
    "    \n",
    "    return x.reshape(-1,3, image_size, image_size),xtest.reshape(-1,3, image_size, image_size)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072) (10000, 3072) (50000,)\n",
      "(3072, 3072)\n",
      "(50000, 3, 32, 32)\n",
      "(10000, 3, 32, 32)\n",
      "[0 2 1 8 1 3 7 5 1]\n",
      "Epoch 0 Loss: 0.6117\n",
      ":  0.8178\n",
      "Epoch 1 Loss: 0.6058\n",
      ":  0.8134\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f2c4c5ca0935>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mwrite_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-f2c4c5ca0935>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Backpropagate the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {} Loss: {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Refer to handout for details.\n",
    "- Build scripts to train your model\n",
    "- Submit your code to Autolab\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "\n",
    "\n",
    "def loading_data():\n",
    "    x_train = np.load('train_feats.npy')\n",
    "    x_test = np.load('test_feats.npy')\n",
    "    y_train = np.load('train_labels.npy')\n",
    "    print(x_train.shape,x_test.shape,y_train.shape)\n",
    "    return x_train, y_train, x_test\n",
    "    \n",
    "def initializer(module):\n",
    "    if hasattr(module, 'bias'):\n",
    "        module.bias.data.zero_()\n",
    "    if hasattr(module, 'weight'):\n",
    "        torch.nn.init.xavier_uniform(module.weight.data)\n",
    "\n",
    "def write_results(predictions, output_file='predictions.txt'):\n",
    "    \"\"\"\n",
    "    Write predictions to file for submission.\n",
    "    File should be:\n",
    "        named 'predictions.txt'\n",
    "        in the root of your tar file\n",
    "    :param predictions: iterable of integers\n",
    "    :param output_file:  path to output file.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        for y in predictions:\n",
    "            f.write(\"{}\\n\".format(y))\n",
    "            \n",
    "def to_variable(tensor):\n",
    "    if torch.cuda.is_available():\n",
    "        tensor = tensor.cuda()\n",
    "    return torch.autograd.Variable(tensor)\n",
    "\n",
    "def main():\n",
    "    x_train, y_train, x_test = loading_data()\n",
    "    x_train, x_test = cifar_10_preprocess(x_train, x_test, image_size=32)\n",
    "    print(x_train.shape)\n",
    "    print(x_test.shape)\n",
    "    print(y_train[1:10])\n",
    "    idx = np.arange(len(x_train))\n",
    "    np.random.shuffle(idx)\n",
    "    x_train = x_train[idx]\n",
    "    y_train = y_train[idx]\n",
    "    y_test = np.zeros(len(x_test))\n",
    "    x,y = x_train[1:45000],y_train[1:45000]\n",
    "    x_val,y_val = x_train[45000:],y_train[45000:]\n",
    "    #x, x_val = p.cifar_10_preprocess(x, x_val, image_size=32)\n",
    "    train_loader = DataLoader(TensorDataset(torch.from_numpy(x),torch.from_numpy(y)),batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(torch.from_numpy(x_val),torch.from_numpy(y_val)),batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(TensorDataset(torch.from_numpy(x_test),torch.from_numpy(y_test)),batch_size=32, shuffle=False)\n",
    "    my_net = all_cnn_module()\n",
    "    #my_net.apply(initializer)\n",
    "    my_net.load_state_dict(torch.load(\"check1.py\"))\n",
    "    my_net.double()\n",
    "    \n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optim = torch.optim.SGD(my_net.parameters(), lr=0.0001, momentum=0.9,weight_decay=0.00001)\n",
    "    if torch.cuda.is_available():\n",
    "        my_net = my_net.cuda()\n",
    "        loss_fn = loss_fn.cuda()\n",
    "    toplot = []\n",
    "    toplot1 = []\n",
    "    for epoch in range(20):\n",
    "        losses = []\n",
    "        my_net.train()\n",
    "        for (input_val, label) in train_loader:\n",
    "            optim.zero_grad()  # Reset the gradients\n",
    "            #input_val = to_tensor(input_val)\n",
    "            #label = to_tensor(label)\n",
    "            prediction = my_net(to_variable(input_val))  # Feed forward\n",
    "            loss = loss_fn(prediction, to_variable(label.long()))  # Compute losses\n",
    "            loss.backward()  # Backpropagate the gradients\n",
    "            losses.append(loss.data.cpu().numpy())\n",
    "            optim.step()  # Update the network\n",
    "        print(\"Epoch {} Loss: {:.4f}\".format(epoch, np.asscalar(np.mean(losses))))\n",
    "        \n",
    "        my_net.eval()\n",
    "        pred5 = []\n",
    "        \n",
    "        for (input_val, label) in val_loader:\n",
    "            prediction = my_net(to_variable(input_val))\n",
    "            pred5.extend(prediction.data.cpu().numpy())\n",
    "        print(\": \",(np.argmax(pred5, axis=1)==y_val).mean())\n",
    "        toplot.append((np.argmax(pred5, axis=1)==y_val).mean())\n",
    "        toplot1.append(np.asscalar(np.mean(losses)))\n",
    "        name1 = \"accuracy_{}: {}, {}\".format(epoch, (np.argmax(pred5, axis=1)==y_val).mean(), np.asscalar(np.mean(losses)))\n",
    "        f= open(\"results_relu1.txt\",\"a+\")\n",
    "        f.write(name1)\n",
    "        f.close()\n",
    "        torch.save(my_net.state_dict(), \"check1.py\")\n",
    "        if((epoch+1) % 10 == 0):\n",
    "            optim = torch.optim.SGD(my_net.parameters(), lr=0.00001, momentum=0.9,weight_decay=0.000001)\n",
    "    pred = []\n",
    "    for (input_val, label) in val_loader:\n",
    "        prediction = my_net(to_variable(input_val))\n",
    "        pred.extend(prediction.data.cpu().numpy())\n",
    "    predd = np.argmax(pred, axis=1)\n",
    "    \n",
    "    \n",
    "    write_results(predd)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# evenly sampled time at 200ms intervals\n",
    "t = np.arange(0., 5., 0.2)\n",
    "\n",
    "# red dashes, blue squares and green triangles\n",
    "plt.plot(t, t, 'r', t, t**2, 'b', t, t**3, 'g')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
